{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fa799c6",
   "metadata": {},
   "source": [
    "Certainly! Let's simplify Naive Bayes:\n",
    "\n",
    "**Naive Bayes:**\n",
    "\n",
    "Naive Bayes is a simple yet powerful probabilistic classifier based on Bayes' theorem with an assumption of independence between features. It's particularly effective for text classification and other classification tasks with categorical features.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1. **Bayes' Theorem:** It calculates the probability of a class given some features using the formula:\n",
    "   \\[\n",
    "   P(y | X) = \\frac{P(X | y) \\cdot P(y)}{P(X)}\n",
    "   \\]\n",
    "   where:\n",
    "   - \\( P(y | X) \\) is the posterior probability of class \\( y \\) given features \\( X \\).\n",
    "   - \\( P(X | y) \\) is the likelihood of features \\( X \\) given class \\( y \\).\n",
    "   - \\( P(y) \\) is the prior probability of class \\( y \\).\n",
    "   - \\( P(X) \\) is the probability of features \\( X \\).\n",
    "\n",
    "2. **Naive Assumption:** It assumes that the presence of a particular feature in a class is independent of the presence of other features. This is a \"naive\" assumption but simplifies the computation greatly.\n",
    "\n",
    "3. **Classifier Training:** Naive Bayes calculates probabilities for each class and selects the class with the highest probability as the prediction.\n",
    "\n",
    "**Types of Naive Bayes:**\n",
    "\n",
    "- **Multinomial Naive Bayes:** Suitable for discrete features (e.g., word counts for text classification).\n",
    "  \n",
    "- **Gaussian Naive Bayes:** Assumes features follow a normal distribution (Gaussian distribution).\n",
    "\n",
    "- **Bernoulli Naive Bayes:** Used for binary feature vectors (e.g., presence or absence of a feature).\n",
    "\n",
    "**Useful when:**\n",
    "\n",
    "- You have categorical or discrete features.\n",
    "- Independence assumption holds reasonably well or provides a good approximation.\n",
    "- You need a fast and efficient classifier for large datasets.\n",
    "\n",
    "\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "- **Simplicity:** Naive Bayes is simple to implement and scales well with large datasets.\n",
    "- **Efficiency:** It's computationally efficient and works well with high-dimensional data.\n",
    "- **Assumption:** The independence assumption can be limiting in some cases, especially when features are not truly independent.\n",
    "\n",
    "Naive Bayes classifiers are widely used in spam filtering, sentiment analysis, and document categorization due to their effectiveness and ease of implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35217f6a",
   "metadata": {},
   "source": [
    "# **Python Implementation Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbc49660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        19\n",
      "  versicolor       1.00      0.92      0.96        13\n",
      "   virginica       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.98      0.97      0.97        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset (example with Iris dataset)\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize Gaussian Naive Bayes classifier\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Train the model\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6582220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
