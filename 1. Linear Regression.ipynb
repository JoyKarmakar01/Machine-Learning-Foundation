{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "294f1c74",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "#### Introduction\n",
    "Linear Regression is a supervised learning algorithm used for predicting a continuous target variable based on one or more predictor variables.\n",
    "\n",
    "#### How Linear Regression Works\n",
    "1. **Line of Best Fit**: The line that best represents the relationship between the independent variables and the dependent variable.\n",
    "2. **Ordinary Least Squares (OLS)**: A method to minimize the sum of squared residuals between the observed and predicted values.\n",
    "\n",
    "#### Advantages\n",
    "- Simple to implement and interpret.\n",
    "- Computationally efficient.\n",
    "- Good for understanding relationships between variables.\n",
    "\n",
    "#### Disadvantages\n",
    "- Assumes a linear relationship between variables.\n",
    "- Sensitive to outliers.\n",
    "- Not suitable for complex relationships.\n",
    "\n",
    "#### Steps to Build a Linear Regression Model\n",
    "1. **Data Preparation**: Clean the data and handle missing values.\n",
    "2. **Train-Test Split**: Split the dataset into training and test sets.\n",
    "3. **Model Training**: Train the linear regression model using the training data.\n",
    "4. **Model Evaluation**: Assess the performance using metrics like R-squared and RMSE.\n",
    "5. **Visualization**: Plot the regression line and residuals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c697e428",
   "metadata": {},
   "source": [
    "#### Python Implementation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0df4d35c",
   "metadata": {},
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('path_to_your_dataset.csv')\n",
    "X = data[['feature1', 'feature2']]  # Independent variables\n",
    "y = data['target']  # Dependent variable\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize linear regression model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'R-squared: {r2}')\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X_test['feature1'], y_test, color='blue')\n",
    "plt.plot(X_test['feature1'], y_pred, color='red', linewidth=2)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764f06ea",
   "metadata": {},
   "source": [
    "Sure! Let's simplify Lasso, Ridge, and Elastic Net regressions:\n",
    "\n",
    "### 1. Lasso Regression:\n",
    "\n",
    "Imagine you're trying to predict something (like house prices) using different factors (like size, number of rooms, location). Lasso regression helps by not only predicting but also deciding which factors are most important.\n",
    "\n",
    "- **How it works**: \n",
    "  - Lasso looks at all the factors and decides which ones to focus on. It might ignore some factors completely if they don't seem important for predicting.\n",
    "  - It does this by penalizing (or reducing) the influence of less important factors. This penalty encourages simpler and more focused predictions.\n",
    "\n",
    "- **Useful when**: \n",
    "  - You have many factors, but only a few really matter for predicting the outcome.\n",
    "  - You want the model to automatically select the most important factors.\n",
    "\n",
    "### 2. Ridge Regression:\n",
    "\n",
    "Now, imagine you want to predict something again, but this time you think all the factors are somewhat important. Ridge regression helps to balance their importance and avoid overemphasizing any single factor.\n",
    "\n",
    "- **How it works**: \n",
    "  - Ridge considers all factors and tries to shrink their influence equally. It prevents any one factor from dominating the prediction too much.\n",
    "  - It does this by adding a small penalty to the size of all factors. This penalty keeps the model stable and prevents extreme predictions.\n",
    "\n",
    "- **Useful when**: \n",
    "  - You believe all factors contribute to the prediction, but you want to control how much each factor affects the outcome.\n",
    "  - You want to avoid the model being overly sensitive to small changes in the data.\n",
    "\n",
    "### 3. Elastic Net Regression:\n",
    "\n",
    "Now, imagine you want the best of both worldsâ€”selecting important factors like Lasso but also balancing their effects like Ridge. Elastic Net combines these approaches to give you a flexible and robust model.\n",
    "\n",
    "- **How it works**: \n",
    "  - Elastic Net combines the feature selection of Lasso (choosing important factors) with the regularization of Ridge (balancing their effects).\n",
    "  - It does this by using two penalties: one like Lasso to select features and another like Ridge to control their sizes.\n",
    "\n",
    "- **Useful when**: \n",
    "  - You have many factors, some of which are correlated (related to each other).\n",
    "  - You want a model that can handle both selecting important factors and keeping their effects balanced.\n",
    "\n",
    "### Comparison:\n",
    "\n",
    "- **Lasso** focuses on selecting important factors and can ignore less important ones completely.\n",
    "  \n",
    "- **Ridge** ensures all factors contribute but prevents any single factor from dominating.\n",
    "\n",
    "- **Elastic Net** combines these approaches, useful when you need both feature selection and balanced effects.\n",
    "\n",
    "These methods are essential in machine learning because they help create models that are not only accurate but also easier to understand and interpret. Depending on your data and goals, you can choose the method that best fits your needs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5387992d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
