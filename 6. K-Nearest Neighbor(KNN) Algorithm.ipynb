{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc30eee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value classified to unknown point is: 0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def classifyAPoint(points,p,k=3):\n",
    "\t'''\n",
    "\tThis function finds the classification of p using\n",
    "\tk nearest neighbor algorithm. It assumes only two\n",
    "\tgroups and returns 0 if p belongs to group 0, else\n",
    "\t1 (belongs to group 1).\n",
    "\n",
    "\tParameters - \n",
    "\t\tpoints: Dictionary of training points having two keys - 0 and 1\n",
    "\t\t\t\tEach key have a list of training data points belong to that \n",
    "\n",
    "\t\tp : A tuple, test data point of the form (x,y)\n",
    "\n",
    "\t\tk : number of nearest neighbour to consider, default is 3 \n",
    "\t'''\n",
    "\n",
    "\tdistance=[]\n",
    "\tfor group in points:\n",
    "\t\tfor feature in points[group]:\n",
    "\n",
    "\t\t\t#calculate the euclidean distance of p from training points \n",
    "\t\t\teuclidean_distance = math.sqrt((feature[0]-p[0])**2 +(feature[1]-p[1])**2)\n",
    "\n",
    "\t\t\t# Add a tuple of form (distance,group) in the distance list\n",
    "\t\t\tdistance.append((euclidean_distance,group))\n",
    "\n",
    "\t# sort the distance list in ascending order\n",
    "\t# and select first k distances\n",
    "\tdistance = sorted(distance)[:k]\n",
    "\n",
    "\tfreq1 = 0 #frequency of group 0\n",
    "\tfreq2 = 0 #frequency og group 1\n",
    "\n",
    "\tfor d in distance:\n",
    "\t\tif d[1] == 0:\n",
    "\t\t\tfreq1 += 1\n",
    "\t\telif d[1] == 1:\n",
    "\t\t\tfreq2 += 1\n",
    "\n",
    "\treturn 0 if freq1>freq2 else 1\n",
    "\n",
    "# driver function\n",
    "def main():\n",
    "\n",
    "\t# Dictionary of training points having two keys - 0 and 1\n",
    "\t# key 0 have points belong to class 0\n",
    "\t# key 1 have points belong to class 1\n",
    "\n",
    "\tpoints = {0:[(1,12),(2,5),(3,6),(3,10),(3.5,8),(2,11),(2,9),(1,7)],\n",
    "\t\t\t1:[(5,3),(3,2),(1.5,9),(7,2),(6,1),(3.8,1),(5.6,4),(4,2),(2,5)]}\n",
    "\n",
    "\t# testing point p(x,y)\n",
    "\tp = (2.5,7)\n",
    "\n",
    "\t# Number of neighbours \n",
    "\tk = 3\n",
    "\n",
    "\tprint(\"The value classified to unknown point is: {}\".\\\n",
    "\t\tformat(classifyAPoint(points,p,k)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15b6339",
   "metadata": {},
   "source": [
    "# Another Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d701fe57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karma\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the KNN classifier with k=3\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb8cba",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) is one of the simplest yet effective machine learning algorithms used for both classification and regression tasks. Here's an overview of KNN, its working principles, advantages, disadvantages, and applications:\n",
    "\n",
    "### What is K-Nearest Neighbors (KNN)?\n",
    "\n",
    "K-Nearest Neighbors is a non-parametric, lazy learning algorithm. Non-parametric means that it makes no explicit assumptions about the form of the function it is trying to learn, and lazy learning means that the algorithm does not learn an explicit model during training but instead stores the training data and performs computations only when it needs to make predictions.\n",
    "\n",
    "### How Does KNN Work?\n",
    "\n",
    "1. **Data Storage**: The algorithm stores all the training data points.\n",
    "2. **Distance Calculation**: When a prediction is required for a new data point, the algorithm calculates the distance between the new data point and all the stored data points. Common distance metrics include Euclidean distance, Manhattan distance, and Minkowski distance.\n",
    "3. **Neighbor Identification**: The algorithm identifies the 'k' training data points that are closest to the new data point.\n",
    "4. **Voting/Prediction**:\n",
    "   - **Classification**: For classification tasks, the algorithm assigns the most common class among the 'k' nearest neighbors to the new data point.\n",
    "   - **Regression**: For regression tasks, the algorithm averages the values of the 'k' nearest neighbors to predict the value for the new data point.\n",
    "\n",
    "### Key Parameters in KNN\n",
    "\n",
    "1. **Number of Neighbors (k)**: The number of nearest neighbors to consider for making the prediction. A small 'k' can lead to noise sensitivity, while a large 'k' can make the algorithm computationally intensive.\n",
    "2. **Distance Metric**: The measure of similarity or dissimilarity between data points. Common metrics include:\n",
    "   - **Euclidean Distance**: \\( \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2} \\)\n",
    "   - **Manhattan Distance**: \\( \\sum_{i=1}^{n} |x_i - y_i| \\)\n",
    "   - **Minkowski Distance**: \\( (\\sum_{i=1}^{n} |x_i - y_i|^p)^{1/p} \\)\n",
    "\n",
    "### Advantages of KNN\n",
    "\n",
    "1. **Simplicity**: KNN is easy to understand and implement.\n",
    "2. **No Training Phase**: Since it is a lazy learner, there is no explicit training phase, which makes it straightforward.\n",
    "3. **Versatility**: Can be used for both classification and regression tasks.\n",
    "4. **Adaptability**: Works well with multi-class problems and can handle both binary and multi-class classification problems.\n",
    "\n",
    "### Disadvantages of KNN\n",
    "\n",
    "1. **Computationally Intensive**: The algorithm can be slow for large datasets since it requires distance calculations for each query point.\n",
    "2. **Memory Intensive**: Requires storing all the training data, which can be impractical for large datasets.\n",
    "3. **Curse of Dimensionality**: As the number of dimensions increases, the distance between points becomes less meaningful, which can degrade the algorithm's performance.\n",
    "4. **Sensitivity to Noise**: KNN is sensitive to noisy data and outliers.\n",
    "\n",
    "### Applications of KNN\n",
    "\n",
    "1. **Image Recognition**: Used in image classification tasks where the similarity between images is measured.\n",
    "2. **Recommendation Systems**: Utilized to recommend products based on the similarity of users' preferences.\n",
    "3. **Finance**: Applied in predicting stock prices and credit risk assessment.\n",
    "4. **Medical Diagnosis**: Used for diagnosing diseases based on patient data and historical records.\n",
    "\n",
    "### Example of KNN in Python\n",
    "\n",
    "Here's a simple example using the `scikit-learn` library to implement KNN for a classification task:\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the KNN classifier with k=3\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "```\n",
    "\n",
    "This example demonstrates how to load a dataset, split it into training and testing sets, train a KNN classifier, make predictions, and evaluate the model's accuracy. Adjusting the number of neighbors (k) and experimenting with different distance metrics can help optimize the model for specific datasets and tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6365d11",
   "metadata": {},
   "source": [
    "Explanation\n",
    "Data Loading: We load the Iris dataset using load_iris() from sklearn.datasets.\n",
    "Data Splitting: We split the dataset into training and testing sets using train_test_split().\n",
    "Model Initialization: We initialize the KNN classifier with n_neighbors=3.\n",
    "Model Training: We train the KNN model using the training data.\n",
    "Predictions: We make predictions on the test data.\n",
    "Evaluation: We evaluate the model's accuracy, print the classification report, and display the confusion matrix.\n",
    "Visualization: We visualize the decision boundaries for the first two features of the dataset.\n",
    "You can modify the number of neighbors (n_neighbors) and experiment with different distance metrics to see how they affect the model's performance. This example provides a solid foundation for understanding and implementing the KNN algorithm in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38b8334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
